---
title: "Lab 1 Block 2"
author: "Damian Ke & Kyriakos Papadopoulos"
date: "`r Sys.Date()`"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Assigment 1
```{r, include=FALSE}
library(randomForest)
library(gridExtra)
library(ggplot2)

# Training 
x1<-runif(100)
x2<-runif(100)
trdata<-cbind(x1,x2)
y<-as.numeric(x1<x2)
trlabels<-as.factor(y)

# Test
set.seed(1234)
x1<-runif(1000)
x2<-runif(1000)
tedata<-cbind(x1,x2)
y<-as.numeric(x1<x2)
telabels<-as.factor(y)
plot(x1,x2,col=(y+1))

# miss class function
missclass=function(X){
  n=sum(X)
  a = sum(diag(X))/n
  return(1-a)
}
```

**Question 1: ** Repeat the procedure above for 1000 training datasets of size 100 and report the mean and variance of the misclassification errors. In other words, create 1000 training datasets of size 100, learn a random forest from each dataset, and compute the misclassification error in the same test dataset of size 1000. Report results for when the random forest has 1, 10 and 100 trees.

**Answer 1: **
```{r, include=FALSE}
# first dot
n = 1000

ms_error1 <- c()
ms_error10 <- c()
ms_error100 <- c()
for(i in 1:n){
  x1<-runif(100)
  x2<-runif(100)
  trdata<-cbind(x1,x2)
  y1<-as.numeric(x1<x2)
  trlabels<-as.factor(y1)
  df <- data.frame(x1, x2, trlabels)
  
  set.seed(1234)
  x1<-runif(1000)
  x2<-runif(1000)
  tedata<-cbind(x1,x2)
  y<-as.numeric(x1<x2)
  telabels<-as.factor(y)

  model1 <- randomForest(trlabels ~ x1 + x2, data = df, nodesize=25, keep.forest = TRUE,
                            ntree = 1)
  
  model2 <- randomForest(trlabels ~ x1 + x2, data = df, nodesize=25, keep.forest = TRUE,
                          ntree = 10)
  
  model3 <- randomForest(trlabels ~ x1 + x2, data = df, nodesize=25, keep.forest = TRUE,
                          ntree = 100)
  
  pred1 <- predict(model1, tedata)
  pred2 <- predict(model2, tedata)
  pred3 <- predict(model3, tedata)

  confusion_matrix_1 = table(y, pred1)
  confusion_matrix_10 = table(y, pred2)
  confusion_matrix_100 = table(y, pred3)
  
  ms_error1 <- append(ms_error1, missclass(confusion_matrix_1))
  ms_error10 <- append(ms_error10, missclass(confusion_matrix_10))
  ms_error100 <- append(ms_error100, missclass(confusion_matrix_100))
  
}
```


```{r, echo=FALSE}

df = data.frame(mean=c(mean(ms_error1), mean(ms_error10), mean(ms_error100)),
                variance=c(var(ms_error1), var(ms_error10), var(ms_error100)))
rownames(df) <- c("K = 1", "K = 10", "K = 100")
knitr::kable(df)
```

**Question 2: ** Repeat the exercise above but this time use the condition (x1<0.5) instead of (x1<x2) when producing the training and test datasets.

**Answer 2: **
```{r, include=FALSE}
set.seed(1234)
x1<-runif(1000)
x2<-runif(1000)
tedata<-cbind(x1,x2)
y<-as.numeric(x1 < 0.5)
telabels<-as.factor(y)

n = 1000

ms2_error1 <- c()
ms2_error10 <- c()
ms2_error100 <- c()
for(i in 1:n){
  x1<-runif(100)
  x2<-runif(100)
  trdata<-cbind(x1,x2)
  y1<-as.numeric(x1 < 0.5)
  trlabels<-as.factor(y1)
  df <- data.frame(x1, x2, trlabels)
  

  model1 <- randomForest(trlabels ~ x1 + x2, data = df, nodesize=25, keep.forest = TRUE,
                         ntree = 1)
  
  model2 <- randomForest(trlabels ~ x1 + x2, data = df, nodesize=25, keep.forest = TRUE,
                         ntree = 10)
  
  model3 <- randomForest(trlabels ~ x1 + x2, data = df, nodesize=25, keep.forest = TRUE,
                         ntree = 100)
  
  pred1 <- predict(model1, tedata)
  pred2 <- predict(model2, tedata)
  pred3 <- predict(model3, tedata)
  
  confusion_matrix_1 = table(telabels, pred1)
  confusion_matrix_10 = table(telabels, pred2)
  confusion_matrix_100 = table(telabels, pred3)
  
  ms2_error1 <- append(ms2_error1, missclass(confusion_matrix_1))
  ms2_error10 <- append(ms2_error10, missclass(confusion_matrix_10))
  ms2_error100 <- append(ms2_error100, missclass(confusion_matrix_100))
}
```

```{r, echo=FALSE}
df = data.frame(mean=c(mean(ms2_error1), mean(ms2_error10), mean(ms2_error100)),
                variance=c(var(ms2_error1), var(ms2_error10), var(ms2_error100)))
rownames(df) <- c("K = 1", "K = 10", "K = 100")
knitr::kable(df)
```

**Question 3: ** Repeat the exercise above but this time use the condition ((x1<0.5 & x2<0.5) | (x1>0.5 & x2>0.5)) instead of (x1<x2) when producing the training and test datasets. Unlike above, use nodesize = 12 for this exercise.

**Answer 3: ** 

```{r, include=FALSE}
set.seed(1234)
x1<-runif(1000)
x2<-runif(1000)
tedata<-cbind(x1,x2)
y<-as.numeric((x1<0.5 & x2<0.5) | (x1>0.5 & x2>0.5))
telabels<-as.factor(y)

n = 1000

ms3_error1 <- c()
ms3_error10 <- c()
ms3_error100 <- c()
for(i in 1:n){
  x1<-runif(100)
  x2<-runif(100)
  trdata<-cbind(x1,x2)
  y1<-as.numeric ((x1<0.5 & x2<0.5) | (x1>0.5 & x2>0.5)) 
  trlabels<-as.factor(y1)
  df <- data.frame(x1, x2, trlabels)
  
  model1 <- randomForest(trlabels ~ x1 + x2, data = df, nodesize=12, keep.forest = TRUE,
                         ntree = 1)
  
  model2 <- randomForest(trlabels ~ x1 + x2, data = df, nodesize=12, keep.forest = TRUE,
                         ntree = 10)
  
  model3 <- randomForest(trlabels ~ x1 + x2, data = df, nodesize=12, keep.forest = TRUE,
                         ntree = 100)
  
  pred1 <- predict(model1, tedata)
  pred2 <- predict(model2, tedata)
  pred3 <- predict(model3, tedata)
  
  confusion_matrix_1 = table(y, pred1)
  confusion_matrix_10 = table(y, pred2)
  confusion_matrix_100 = table(y, pred3)
  
  ms3_error1 <- append(ms3_error1, missclass(confusion_matrix_1))
  ms3_error10 <- append(ms3_error10, missclass(confusion_matrix_10))
  ms3_error100 <- append(ms3_error100, missclass(confusion_matrix_100))
}
```

```{r, echo=FALSE}
df = data.frame(mean=c(mean(ms3_error1), mean(ms3_error10), mean(ms3_error100)),
                variance=c(var(ms3_error1), var(ms3_error10), var(ms3_error100)))
rownames(df) <- c("K = 1", "K = 10", "K = 100")
knitr::kable(df)
```

**Question 4: ** What happens with the mean error rate when the number of trees in the random forest grows? Why?

**Answer 4: ** We can see that in all 3 cases mean error decreases when the number
of trees gets bigger. But, the big decrease in terms of error is from K=1 to K=10.
The error decreases from K=10 to K=100, but not so much as previously. This is can be easily explained from slide 8 at lecture Lecture1aBlock22021. We can see that for random forest as we increase the number of trees we get a smaller error. but after a point the error becomes almost stable and don't see any big difference to error even if we increase the number of trees a lot. That's what happens from K=10 to K=100. 

**Question 5: ** The third dataset represents a slightly more complicated classification problem than the first one. Still, you should get better performance for it when using sufficient trees in the random forest. Explain why you get better performance.

**Answer 5: **

```{r, echo=FALSE, fig.align='center'}
par(mfrow=c(1,2))

# Visualazing test dataset 1
set.seed(1234)
x1<-runif(1000)
x2<-runif(1000)
tedata<-cbind(x1,x2)
y<-as.numeric(x1<x2)
telabels<-as.factor(y)
plot(x1,x2,col=(y+1), main = "x1 < x2")

# Visualazing test dataset 3
set.seed(1234)
x1 <- runif(1000)
x2 <- runif(1000)
tedata <- cbind(x1,x2)
y <- as.numeric((x1<0.5 & x2<0.5) | (x1>0.5 & x2>0.5))
telabels <- as.factor(y)
plot(x1,x2,col=(y+1), main = "(x1<0.5 & x2<0.5) | (x1>0.5 & x2>0.5)")


```

The reason that we get better performance when using sufficient trees in the third dataset compared to the first dataset, it's because the first dataset is much simpler. As we can see, we can seperate almost perfectly the data in the first dataset with only one diagonical line. On the other hand, the third dataset can not be seperated so good as it's more complicated. So, when the number of trees increases the model becomes more complicated so that's why we have a bigger error in for k=10 and k=100 for the first dataset compared to the third one. For that number of trees the model is very complicated to handle a so simple dataset like the first one. One more reason is that we reduced the node size from 25 to 12 for the third dataset. According the to the documentation of Randomforest we can see that the nodesize defines the minimum size of terminal nodes. Setting this number larger causes smaller trees to be grown. With a bigger tree we will get a more complicated model so with a more complicated model we can make better predictions on a complicated dataset like the third one.


## Assigment 2

**Explanation of the added code**:
First part of code calculates $p(X,y|\theta)$ which is also equal to the weight $w_i(m)$.

According to the course literature, EM-method has 2 steps.

**Step E**: Requires to compute the $Q(\theta)$ which corresponds for
$\Sigma_y ln(p(X,y|\theta))*p(y|X,\hat{\theta})$
and can be explained as $\Sigma_yln(p(X,y|\theta)))*w_i(m)$.
Which is the second part of the code.

**Step M**: Update $\hat{\theta} \leftarrow arg max_\theta$. $Q(\theta)$.
Which corresponds for this case to update $\hat\mu_m$ and $\hat\pi_m$ and fourth part of code.

These steps are iterated until convergence. Which is why there is break statement if the changes
between the iterations have not changed significantly and corresponds for third part of code.


```{r, echo=FALSE}
for (m_values in 2:4){
  set.seed(1234567890)
  max_it <- 100 # max number of EM iterations
  min_change <- 0.1 # min change in log lik between two consecutive iterations
  n=1000 # number of training points
  D=10 # number of dimensions
  x <- matrix(nrow=n, ncol=D) # training data
  true_pi <- vector(length = 3) # true mixing coefficients
  true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
  true_pi=c(1/3, 1/3, 1/3)
  true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
  true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
  true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
  # plot(true_mu[1,], type="o", col="blue", ylim=c(0,1))
  # points(true_mu[2,], type="o", col="red")
  # points(true_mu[3,], type="o", col="green")
  # Producing the training data
  for(i in 1:n) {
    m <- sample(1:3,1,prob=true_pi)
    for(d in 1:D) {
      x[i,d] <- rbinom(1,1,true_mu[m,d])
    }
  }
  M=m_values # number of clusters
  w <- matrix(nrow=n, ncol=M) # weights
  pi <- vector(length = M) # mixing coefficients
  mu <- matrix(nrow=M, ncol=D) # conditional distributions
  llik <- vector(length = max_it) # log likelihood of the EM iterations
  # Random initialization of the parameters
  pi <- runif(M,0.49,0.51)
  pi <- pi / sum(pi)
  for(m in 1:M) {
    mu[m,] <- runif(D,0.49,0.51)
  }
  pi
  mu

  for(it in 1:max_it) {
    Sys.sleep(0.5)
    # E-step: Computation of the weights
    for (N in 1:n){
      px = c()
      for (m in 1:M){
        bern = 1
        for (d in 1:D){
          bern = bern * (mu[m,d]**x[N,d])*((1-mu[m,d])**(1-x[N,d]))
        }
        px[m]= (pi[m] * bern)
      }
      for (m in 1:M){
        w[N,m] = px[m]/sum(px) 
      }
    }
    #Log likelihood computation.
    difference = 0
    for (N in 1:n){
      logpx = c()
      for (m in 1:M){
        logbern = 0
        for (d in 1:D){
          logbern = logbern + x[N,d]*log(mu[m,d])+(1-x[N,d])*log(1-mu[m,d])
        }
        logpx[m]= log(pi[m]) + logbern
        difference = difference + w[N,m]*logpx[m]
      }
    }
    llik[it] = difference
    #cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
    #flush.console()
    # Stop if the lok likelihood has not changed significantly
    if(it > 1){
      if((llik[it] - llik[it - 1]) <= min_change){
        break
      }
    }
    #M-step: ML parameter estimation from the data and weights
    mu <- t(w) %*% x/colSums(w)
    # Pi calulcation
    pi <- colSums(w) / nrow(x)
  }
  
  if(M==2){
    df_mu = data.frame(mu[1,],mu[2,],1:D)
    df_llik = data.frame(1:it,llik[1:it])
    colnames(df_mu) =c("mu_1","mu_2","Dimensions")
    colnames(df_llik) = c("Iterations","llik_iterations")
    p_1 = ggplot(df_mu, aes(x=Dimensions))+
      geom_line(aes(y=mu_1),color="red")+
      geom_line(aes(y=mu_2),color="blue")+
      ylab("Mu")+ggtitle("Mu for M=2")
    
    pl_1 = ggplot(df_llik, aes(x=Iterations))+
      geom_line(aes(y=llik_iterations))+
      ylab("Loglikelihood")+ggtitle("M=2")
  }
  else if(M==3){
    df_mu = data.frame(mu[1,],mu[2,],mu[3,],1:D)
    df_llik = data.frame(1:it,llik[1:it])
    colnames(df_mu) =c("mu_1","mu_2","mu_3","Dimensions")
    colnames(df_llik) = c("Iterations","llik_iterations")
    p_2 = ggplot(df_mu, aes(x=Dimensions))+
      geom_line(aes(y=mu_1),color="red")+
      geom_line(aes(y=mu_2),color="blue")+
      geom_line(aes(y=mu_3))+
      ylab("Mu")+ggtitle("Mu for M=3")
    
    pl_2 = ggplot(df_llik, aes(x=Iterations))+
      geom_line(aes(y=llik_iterations))+
      ylab("Loglikelihood")+ggtitle("M=3")
    
  }
  else if(M==4){
    df_mu = data.frame(mu[1,],mu[2,],mu[3,],mu[4,],1:D)
    df_llik = data.frame(1:it,llik[1:it])
    colnames(df_mu) =c("mu_1","mu_2","mu_3","mu_4","Dimensions")
    colnames(df_llik) = c("Iterations","llik_iterations")
    p_3 = ggplot(df_mu, aes(x=Dimensions))+
      geom_line(aes(y=mu_1),color="red")+
      geom_line(aes(y=mu_2),color="blue")+
      geom_line(aes(y=mu_3))+
      geom_line(aes(y=mu_4),color="cyan")+
      ylab("Mu")+ggtitle("Mu for M=4")
    
    pl_3 = ggplot(df_llik, aes(x=Iterations))+
      geom_line(aes(y=llik_iterations))+
      ylab("Loglikelihood")+ggtitle("M=4")
    
  }
  df_true_mu = data.frame(true_mu[1,],true_mu[2,],true_mu[3,],1:D)
  colnames(df_true_mu) =c("mu_1","mu_2","mu_3","Dimensions")
  p_true = ggplot(df_true_mu, aes(x=Dimensions))+
      geom_line(aes(y=mu_1),color="red")+
      geom_line(aes(y=mu_2),color="blue")+
      geom_line(aes(y=mu_3))+
      ylab("Mu")+ggtitle("True Mu")

}
grid.arrange(p_1, p_2,p_3, ncol=3)
grid.arrange(pl_1, pl_2,pl_3, ncol=3)
p_true

```

As it can be seen in the figure, for higher number of M the mu values
start to follow unique pattern. When M=4, the line corresponding to color blue and red is different from the other lines.
The number of iterations differs only for M=2, where 16 iterations were required.
For M=3 and M=4 are around 60 iterations.
It can also be seen that at around 5 iterations there is a large increase of the value. Thereafter, it converges to around 6500 which is
the stationary point and the local maximum.


According to the given code there are 3 lines.
Two of the lines follow the same pattern of all of the cases with very small differences.
For M=2, there are one less lines defined than the actual number of lines therefore they are not exactly same as in M=3. 
The lines are following the same pattern as in M=3. But the third line needs to be divided into the two lines, therefore they are
quite different.
For M=4, there is additional one line added than the actual number of lines which means that an additional line needs to be added. 
The combination of red and blue line corresponds to the blue line in M=3.

# Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
